{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s55oVTgTXTDP"
      },
      "source": [
        "# Lab 11\n",
        "\n",
        "This code is from https://pytorch.org/tutorials/beginner/transformer_tutorial.html, the official pytorch documentation for seq2seq modeling using the Transformer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-image"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfF_8Bf9dydj",
        "outputId": "1df9a355-30e4-4578-e512-6c86e46f8d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.10.1)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (3.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (8.4.0)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2.25.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (2023.4.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _supported_float_type(input_dtype, allow_complex=False):\n",
        "    if isinstance(input_dtype, Iterable) and not isinstance(input_dtype, str):\n",
        "        return np.result_type(*(_supported_float_type(d) for d in input_dtype))\n",
        "    input_dtype = np.dtype(input_dtype)\n",
        "    if not allow_complex and input_dtype.kind == 'c':\n",
        "        raise ValueError(\"complex valued input is not supported\")\n",
        "    return new_float_type.get(input_dtype.char, np.float64)\n",
        "\n",
        "def _generic_edge_filter(image, *, smooth_weights, edge_weights=[1, 0, -1],\n",
        "                         axis=None, mode='reflect', cval=0.0, mask=None):\n",
        "    ndim = image.ndim\n",
        "    if axis is None:\n",
        "        axes = list(range(ndim))\n",
        "    elif np.isscalar(axis):\n",
        "        axes = [axis]\n",
        "    else:\n",
        "        axes = axis\n",
        "    return_magnitude = (len(axes) > 1)\n",
        "\n",
        "    if image.dtype.kind == 'f':\n",
        "        float_dtype = _supported_float_type(image.dtype)\n",
        "        image = image.astype(float_dtype, copy=False)\n",
        "    else:\n",
        "        image = img_as_float(image)\n",
        "    output = np.zeros(image.shape, dtype=image.dtype)\n",
        "\n",
        "    for edge_dim in axes:\n",
        "        kernel = _reshape_nd(edge_weights, ndim, edge_dim)\n",
        "        smooth_axes = list(set(range(ndim)) - {edge_dim})\n",
        "        for smooth_dim in smooth_axes:\n",
        "            kernel = kernel * _reshape_nd(smooth_weights, ndim, smooth_dim)\n",
        "        ax_output = ndi.convolve(image, kernel, mode=mode)\n",
        "        if return_magnitude:\n",
        "            ax_output *= ax_output\n",
        "        output += ax_output\n",
        "\n",
        "    if return_magnitude:\n",
        "        output = np.sqrt(output) / np.sqrt(ndim)\n",
        "    return output"
      ],
      "metadata": {
        "id": "9s4dQh1if8Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import filters\n",
        "import numpy as np\n",
        "\n",
        "seed = np.random.seed(7777)\n",
        "image = np.random.rand(128,512,512)\n",
        "edges = filters.sobel(image)"
      ],
      "metadata": {
        "id": "tEqOrh6cd_9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtb2-yhjf9C9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67dcffa6-4dcc-4168-c6ba-7a34be20ce96"
      },
      "source": [
        "!pip3 install torchtext==0.4.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.4.0\n",
            "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.4.0) (1.16.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.4.0) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.4.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.4.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.4.0) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.4.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.4.0) (1.3.0)\n",
            "Installing collected packages: torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.1\n",
            "    Uninstalling torchtext-0.15.1:\n",
            "      Successfully uninstalled torchtext-0.15.1\n",
            "Successfully installed torchtext-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XjsunFtWPQv"
      },
      "source": [
        "\n",
        "Sequence-to-Sequence Modeling with nn.Transformer and TorchText\n",
        "===============================================================\n",
        "\n",
        "This is a tutorial on how to train a sequence-to-sequence model\n",
        "that uses the\n",
        "[nn.Transformer](https://pytorch.org/docs/master/nn.html?highlight=nn%20transformer#torch.nn.Transformer) module.\n",
        "\n",
        "The PyTorch 1.2 release includes a standard transformer module based on the\n",
        "paper [Attention is All You\n",
        "Need](https://arxiv.org/pdf/1706.03762.pdf) . The transformer model\n",
        "has been proved to be superior in quality for many sequence-to-sequence\n",
        "problems while being more parallelizable. The ``nn.Transformer`` module\n",
        "relies entirely on an attention mechanism (implemented in [nn.MultiheadAttention](https://pytorch.org/docs/master/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention) to draw global dependencies\n",
        "between input and output. The ``nn.Transformer`` module is highly\n",
        "modularized. As a result, a single component (like `nn.TransformerEncoder`\n",
        "in this tutorial) can be easily adapted/composed.\n",
        "\n",
        "The figure below is an overall summary of the transformer architecture.\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/transformer_architecture.jpg?raw=1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23r1Mv0wWPQw"
      },
      "source": [
        "Define the model\n",
        "----------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K102Br-MWPQx"
      },
      "source": [
        "In this tutorial, we train a ``nn.TransformerEncoder`` model on a\n",
        "language modeling task. One formulation of language modeling is to assign a\n",
        "probability for the likelihood of a given word (or a sequence of words)\n",
        "to follow a given sequence of words. \n",
        "\n",
        "A sequence of tokens are passed to the embedding\n",
        "layer first, followed by a positional encoding layer to account for the order\n",
        "of the words (see the next paragraph for more details). The\n",
        "``nn.TransformerEncoder`` module consists of multiple layers of\n",
        "[nn.TransformerEncoderLayer](https://pytorch.org/docs/master/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer). Along with the input sequence, a square\n",
        "attention mask is required because the self-attention layers in\n",
        "``nn.TransformerEncoder`` are only allowed to attend to earlier positions in\n",
        "the sequence. For language modeling, any tokens in future\n",
        "positions should be masked. To get the actual words, the output\n",
        "of the ``nn.TransformerEncoder`` model is sent to a final Linear\n",
        "layer, which is followed by a log-Softmax function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY8FTTcSa_f7"
      },
      "source": [
        "### Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "po8nLNxXWPQy"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
        "        '''\n",
        "        ntoken/ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "        ninp/emsize = 200 # embedding dimension\n",
        "        nhead = 2 # the number of heads in the multiheadattention models\n",
        "        nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "        nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "        dropout = 0.2 # the dropout value\n",
        "        '''\n",
        "        super(TransformerModel, self).__init__()\n",
        "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "        self.model_type = 'Transformer'\n",
        "        self.src_mask = None\n",
        "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        self.ninp = ninp\n",
        "        self.decoder = nn.Linear(ninp, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def _generate_square_subsequent_mask(self, sz):\n",
        "        #triu returns the upper triangular part of a matrix (2-D tensor) or batch of matrices (see section below)\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "            device = src.device\n",
        "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "            self.src_mask = mask\n",
        "\n",
        "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, self.src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYv1_CYrJzFG"
      },
      "source": [
        "#### Masking\n",
        "By passing the mask into the transformer_encoder forward() function, attention will only be calculated based on the earlier positions in the sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKVBR--Galxk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "238a5a46-2e85-4b93-b0bb-262d5a3dc272"
      },
      "source": [
        "#triu returns the upper triangular part of a matrix (2-D tensor) or batch of matrices (see section below)\n",
        "torch.triu(torch.ones(3, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [0., 1., 1.],\n",
              "        [0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Masking\n",
        "def masking():\n",
        "  sz = 4\n",
        "  mask = (torch.triu(torch.ones(sz, sz)) == 1)\n",
        "  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "  \n",
        "  return mask\n",
        "\n",
        "masking()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tobhlNSnRIdU",
        "outputId": "9f82fc3f-d616-458a-f3ec-f12442f7c6ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [-inf, 0., 0., 0.],\n",
              "        [-inf, -inf, 0., 0.],\n",
              "        [-inf, -inf, -inf, 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btnWrvo-ayP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f66c3c-e115-41c7-9bdb-343e1e7843a9"
      },
      "source": [
        "# Masking\n",
        "def masking():\n",
        "  sz = 4\n",
        "  mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "  ## mask = (torch.triu(torch.ones(sz, sz)) == 0)\n",
        "  mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "  \n",
        "  return mask\n",
        "\n",
        "masking()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf],\n",
              "        [0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh47UCwObCf-"
      },
      "source": [
        "### Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xdCyMPQWPQ3"
      },
      "source": [
        "The Transformer architecture follows the base architecture of a Seq2Seq model (Encoder - Decoder). However, the transformer does not use a recurrent model so this means we need a way to captures sequence information in the input and output.\n",
        "\n",
        "The ``PositionalEncoding`` module injects some information about the\n",
        "relative or absolute position of the tokens in the sequence. The\n",
        "positional encodings have the same dimensions as the embeddings so that\n",
        "the two can be summed. Here, we use ``sine`` and ``cosine`` functions of\n",
        "different frequencies.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffk_fy_eWPQ4"
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        '''\n",
        "        d_model = 200 # embedding dimension\n",
        "        max_len = 5000 # the maximum sentence length\n",
        "        '''\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model) ## a matrix of shape [max_len,d_model] with all zeros\n",
        "        ### each row of pe represents one possible position with d_model embedding dimension\n",
        "        ### i.e., each word is represented by d_model embedding dimension, and each possible position is also represented by d_model embedding dimension\n",
        "        #####     so, we can add the word embedding and the positional encoding element-wise\n",
        "        ##### e.g., the third word 'MONEY' is represented by [1,2,...,200], and the third position in a sentence is represented by [1,2,...,200]\n",
        "        #####        then, the 'MONEY' inputted to transformer-encoder is [2,4,...,400] \n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        ## torch.arange(start,end,step=1) returns a 1-D tensor of size ((end-start)/step) with values \n",
        "        ### from the interval [start,end) taken with common difference step begining from start\n",
        "        ## .unsqueeze(1) returns a new tensor with a dimension of size one inserted at the specified position.\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term) #0::2 means starting with index 0, step = 2\n",
        "        pe[:, 1::2] = torch.cos(position * div_term) #1::2 means starting with index 1, step = 2\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "        ## If you have parameters in your model, which should be saved and restored in the state_dict, but not trained by the optimizer, you should register them as buffers.\n",
        "        ## Buffers won’t be returned in model.parameters(), so that the optimizer won’t have a change to update them.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :] # x.size(0)=the length of the sentence\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65glmcV4WPQ9"
      },
      "source": [
        "Load and batch data\n",
        "-------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHI1s_A_WPQ-"
      },
      "source": [
        "The training process uses Wikitext-2 dataset from ``torchtext``. The\n",
        "vocab object is built based on the training dataset and is used to numericalize\n",
        "tokens into tensors. Starting from sequential data, the ``batchify()``\n",
        "function arranges the dataset into columns, trimming off any tokens remaining\n",
        "after the data has been divided into batches of size ``batch_size``.\n",
        "For instance, with the alphabet as the sequence (total length of 26)\n",
        "and a batch size of 4, we would divide the alphabet into 4 sequences of\n",
        "length 6:\n",
        "\n",
        "\\begin{align}\\begin{bmatrix}\n",
        "  \\text{A} & \\text{B} & \\text{C} & \\ldots & \\text{X} & \\text{Y} & \\text{Z}\n",
        "  \\end{bmatrix}\n",
        "  \\Rightarrow\n",
        "  \\begin{bmatrix}\n",
        "  \\begin{bmatrix}\\text{A} \\\\ \\text{B} \\\\ \\text{C} \\\\ \\text{D} \\\\ \\text{E} \\\\ \\text{F}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{G} \\\\ \\text{H} \\\\ \\text{I} \\\\ \\text{J} \\\\ \\text{K} \\\\ \\text{L}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{M} \\\\ \\text{N} \\\\ \\text{O} \\\\ \\text{P} \\\\ \\text{Q} \\\\ \\text{R}\\end{bmatrix} &\n",
        "  \\begin{bmatrix}\\text{S} \\\\ \\text{T} \\\\ \\text{U} \\\\ \\text{V} \\\\ \\text{W} \\\\ \\text{X}\\end{bmatrix}\n",
        "  \\end{bmatrix}\\end{align}\n",
        "\n",
        "These columns are treated as independent by the model, which means that\n",
        "the dependence of ``G`` and ``F`` can not be learned, but allows more\n",
        "efficient batch processing.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bJTXDCtWPQ_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b44eb11f-a640-45d4-9c76-12fe1a5a8ac0"
      },
      "source": [
        "import torchtext\n",
        "import torch.utils.data.DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
        "                            init_token='<sos>',\n",
        "                            eos_token='<eos>',\n",
        "                            lower=True)\n",
        "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
        "TEXT.build_vocab(train_txt)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def batchify(data, bsz):\n",
        "    data = TEXT.numericalize([data.examples[0].text])\n",
        "    # Divide the dataset into bsz parts.\n",
        "    nbatch = data.size(0) // bsz\n",
        "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
        "    data = data.narrow(0, 0, nbatch * bsz)\n",
        "    # Evenly divide the data across the bsz batches.\n",
        "    data = data.view(bsz, -1).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_txt, batch_size)\n",
        "val_data = batchify(val_txt, eval_batch_size)\n",
        "test_data = batchify(test_txt, eval_batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading wikitext-2-v1.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "wikitext-2-v1.zip: 100%|██████████| 4.48M/4.48M [00:00<00:00, 18.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0OE4TAV9WPRD"
      },
      "source": [
        "## Functions to generate input and target sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kq_pEBjoWPRE"
      },
      "source": [
        "The ``get_batch()`` function generates the input and target sequence for\n",
        "the transformer model. It subdivides the source data into chunks of\n",
        "length ``bptt``. For the language modeling task, the model needs the\n",
        "following words as ``Target``. For example, with a ``bptt`` value of 2,\n",
        "we’d get the following two Variables for ``i`` = 0:\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_static/img/transformer_input_target.png?raw=1)\n",
        "\n",
        "\n",
        "It should be noted that the chunks are along dimension 0, consistent\n",
        "with the ``S`` dimension in the Transformer model. The batch dimension\n",
        "``N`` is along dimension 1.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPhRYAjYWPRE"
      },
      "source": [
        "bptt = 35\n",
        "def get_batch(source, i):\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].view(-1)\n",
        "    return data, target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6_5USduWPRU"
      },
      "source": [
        "Initiate an instance\n",
        "--------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr6Wa55XWPRV"
      },
      "source": [
        "The model is set up with the hyperparameter below. The vocab size is\n",
        "equal to the length of the vocab object.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky6yqdrkWPRW"
      },
      "source": [
        "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
        "emsize = 200 # embedding dimension\n",
        "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2 # the number of heads in the multiheadattention models\n",
        "dropout = 0.2 # the dropout value\n",
        "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGCV6PTHWPRd"
      },
      "source": [
        "Run the model\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7nw5RbXWPRf"
      },
      "source": [
        "`CrossEntropyLoss `\n",
        "is applied to track the loss and\n",
        "`SGD `\n",
        "implements stochastic gradient descent method as the optimizer. The initial\n",
        "learning rate is set to 5.0. `StepLR ` is\n",
        "applied to adjust the learn rate through epochs. During\n",
        "training, we use the\n",
        "`nn.utils.clip_grad_norm_ `\n",
        "function to scale all the gradients together to avoid the exploding gradient problem.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FrOT_epWPRg"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0 # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "import time\n",
        "def train():\n",
        "\n",
        "    model.train() # Turn on the train mode\n",
        "    total_loss = 0.\n",
        "    start_time = time.time()\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        log_interval = 200\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
        "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
        "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
        "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
        "                    elapsed * 1000 / log_interval,\n",
        "                    cur_loss, math.exp(cur_loss)))\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "def evaluate(eval_model, data_source):\n",
        "    eval_model.eval() # Turn on the evaluation mode\n",
        "    total_loss = 0.\n",
        "    ntokens = len(TEXT.vocab.stoi)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i)\n",
        "            output = eval_model(data)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(data_source) - 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_dj-0RwWPRj"
      },
      "source": [
        "Loop over epochs. Save the model if the validation loss is the best\n",
        "we've seen so far. Adjust the learning rate after each epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n17HNUS4WPRk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "027cce6d-dfe2-4bbd-b86a-85b2852dcd72"
      },
      "source": [
        "best_val_loss = float(\"inf\")\n",
        "epochs = 3 # The number of epochs\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train()\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    print('-' * 89)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
        "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
        "                                     val_loss, math.exp(val_loss)))\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "\n",
        "    scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:389: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/ 2981 batches | lr 5.00 | ms/batch 29.40 | loss  8.04 | ppl  3111.89\n",
            "| epoch   1 |   400/ 2981 batches | lr 5.00 | ms/batch 14.16 | loss  6.78 | ppl   877.50\n",
            "| epoch   1 |   600/ 2981 batches | lr 5.00 | ms/batch 13.48 | loss  6.37 | ppl   581.48\n",
            "| epoch   1 |   800/ 2981 batches | lr 5.00 | ms/batch 13.61 | loss  6.22 | ppl   504.30\n",
            "| epoch   1 |  1000/ 2981 batches | lr 5.00 | ms/batch 13.55 | loss  6.12 | ppl   454.53\n",
            "| epoch   1 |  1200/ 2981 batches | lr 5.00 | ms/batch 13.98 | loss  6.09 | ppl   439.72\n",
            "| epoch   1 |  1400/ 2981 batches | lr 5.00 | ms/batch 13.80 | loss  6.04 | ppl   419.91\n",
            "| epoch   1 |  1600/ 2981 batches | lr 5.00 | ms/batch 13.62 | loss  6.05 | ppl   424.26\n",
            "| epoch   1 |  1800/ 2981 batches | lr 5.00 | ms/batch 13.62 | loss  5.96 | ppl   387.72\n",
            "| epoch   1 |  2000/ 2981 batches | lr 5.00 | ms/batch 13.73 | loss  5.95 | ppl   384.22\n",
            "| epoch   1 |  2200/ 2981 batches | lr 5.00 | ms/batch 14.35 | loss  5.85 | ppl   346.17\n",
            "| epoch   1 |  2400/ 2981 batches | lr 5.00 | ms/batch 13.75 | loss  5.89 | ppl   363.18\n",
            "| epoch   1 |  2600/ 2981 batches | lr 5.00 | ms/batch 13.73 | loss  5.90 | ppl   364.34\n",
            "| epoch   1 |  2800/ 2981 batches | lr 5.00 | ms/batch 13.81 | loss  5.80 | ppl   330.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time: 46.49s | valid loss  5.75 | valid ppl   314.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/ 2981 batches | lr 4.51 | ms/batch 14.52 | loss  5.80 | ppl   331.24\n",
            "| epoch   2 |   400/ 2981 batches | lr 4.51 | ms/batch 13.90 | loss  5.78 | ppl   322.36\n",
            "| epoch   2 |   600/ 2981 batches | lr 4.51 | ms/batch 14.01 | loss  5.60 | ppl   271.38\n",
            "| epoch   2 |   800/ 2981 batches | lr 4.51 | ms/batch 14.42 | loss  5.64 | ppl   282.68\n",
            "| epoch   2 |  1000/ 2981 batches | lr 4.51 | ms/batch 13.97 | loss  5.59 | ppl   267.07\n",
            "| epoch   2 |  1200/ 2981 batches | lr 4.51 | ms/batch 14.10 | loss  5.62 | ppl   276.92\n",
            "| epoch   2 |  1400/ 2981 batches | lr 4.51 | ms/batch 14.06 | loss  5.63 | ppl   279.73\n",
            "| epoch   2 |  1600/ 2981 batches | lr 4.51 | ms/batch 14.39 | loss  5.66 | ppl   288.48\n",
            "| epoch   2 |  1800/ 2981 batches | lr 4.51 | ms/batch 14.33 | loss  5.59 | ppl   267.19\n",
            "| epoch   2 |  2000/ 2981 batches | lr 4.51 | ms/batch 14.13 | loss  5.61 | ppl   274.22\n",
            "| epoch   2 |  2200/ 2981 batches | lr 4.51 | ms/batch 14.11 | loss  5.51 | ppl   246.19\n",
            "| epoch   2 |  2400/ 2981 batches | lr 4.51 | ms/batch 15.59 | loss  5.58 | ppl   265.06\n",
            "| epoch   2 |  2600/ 2981 batches | lr 4.51 | ms/batch 16.72 | loss  5.59 | ppl   267.57\n",
            "| epoch   2 |  2800/ 2981 batches | lr 4.51 | ms/batch 14.34 | loss  5.51 | ppl   247.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time: 45.09s | valid loss  5.56 | valid ppl   260.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/ 2981 batches | lr 4.29 | ms/batch 14.91 | loss  5.55 | ppl   256.64\n",
            "| epoch   3 |   400/ 2981 batches | lr 4.29 | ms/batch 14.45 | loss  5.55 | ppl   257.95\n",
            "| epoch   3 |   600/ 2981 batches | lr 4.29 | ms/batch 14.41 | loss  5.37 | ppl   214.82\n",
            "| epoch   3 |   800/ 2981 batches | lr 4.29 | ms/batch 14.40 | loss  5.42 | ppl   226.18\n",
            "| epoch   3 |  1000/ 2981 batches | lr 4.29 | ms/batch 14.71 | loss  5.37 | ppl   215.79\n",
            "| epoch   3 |  1200/ 2981 batches | lr 4.29 | ms/batch 17.55 | loss  5.41 | ppl   222.65\n",
            "| epoch   3 |  1400/ 2981 batches | lr 4.29 | ms/batch 14.42 | loss  5.43 | ppl   229.24\n",
            "| epoch   3 |  1600/ 2981 batches | lr 4.29 | ms/batch 14.39 | loss  5.47 | ppl   237.71\n",
            "| epoch   3 |  1800/ 2981 batches | lr 4.29 | ms/batch 14.58 | loss  5.40 | ppl   220.79\n",
            "| epoch   3 |  2000/ 2981 batches | lr 4.29 | ms/batch 14.58 | loss  5.43 | ppl   228.23\n",
            "| epoch   3 |  2200/ 2981 batches | lr 4.29 | ms/batch 14.29 | loss  5.32 | ppl   204.07\n",
            "| epoch   3 |  2400/ 2981 batches | lr 4.29 | ms/batch 14.28 | loss  5.39 | ppl   219.53\n",
            "| epoch   3 |  2600/ 2981 batches | lr 4.29 | ms/batch 14.28 | loss  5.41 | ppl   222.75\n",
            "| epoch   3 |  2800/ 2981 batches | lr 4.29 | ms/batch 14.75 | loss  5.33 | ppl   207.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time: 45.76s | valid loss  5.50 | valid ppl   244.86\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stKeoW99WPRn"
      },
      "source": [
        "## Evaluate the model with the test dataset\n",
        "\n",
        "Measure results on the test set for the best model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxzbDUnFWPRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312ae341-c1fd-4912-d49c-c7709006d650"
      },
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "print('=' * 89)\n",
        "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
        "    test_loss, math.exp(test_loss)))\n",
        "print('=' * 89)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.41 | test ppl   222.67\n",
            "=========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC3ND0e8I4Z9"
      },
      "source": [
        "# HuggingFace Transformer\n",
        "\n",
        "HuggingFace is a company that develops software and services for NLP. They have a widely used library providing transformers and related models. A large set of tutorial links can be found here: https://huggingface.co/transformers/notebooks.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4xteNApjplg"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edjjkL_SjrL0"
      },
      "source": [
        "## E1. Multiple Choice Questions\n",
        "\n",
        "Please answer the two questions below. Note - you must get both right to get a point."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        "\n",
        "Which of the following are true of attention?\n",
        "\n",
        "1. It allows the decoder in a sequence-to- sequence model to use information from specific parts of the input.\n",
        "2. It is a form of weighted sum.\n",
        "3. It is a key part of the Long Short-Term Memory model (LSTM).\n",
        "4. It is a key part of the Transformer model.\n",
        "5. It is critical for accurate POS tagging.\n",
        "6. It is critical for accurate translation."
      ],
      "metadata": {
        "id": "tCrwfVXbk8nT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n"
      ],
      "metadata": {
        "id": "MYiVWN1yk84T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "\n",
        "Do each of the following text representation methods produce sparse or dense vectors?"
      ],
      "metadata": {
        "id": "nPIj56pEk9Kt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:\n",
        "\n",
        "- Bag of words - \n",
        "- Dynamic Embedding (e.g., BERT) -\n",
        "- One-hot - \n",
        "- Static Embedding (e.g., GloVe) -\n",
        "- TF-IDF -"
      ],
      "metadata": {
        "id": "L2tzlgC6k9cL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdmzzhI5iP93"
      },
      "source": [
        "## E2 Test with Transformer\n",
        "Try either:\n",
        "\n",
        "1. varying the number of heads in Multi-head Attention, **or**\n",
        "2. varying the number of encoder layers\n",
        "\n",
        "Record the test performance for each configuration you try.\n",
        "\n",
        "Draw a graph to show the test performance (or validation losses/ppls) vs number of heads, or the test performance (or validation losses/ppls) vs number of encoders, (you can keep epochs = 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L37XqRug6mKf"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr1tjBrpq8Y5"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9P6lHtMq_9G"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}