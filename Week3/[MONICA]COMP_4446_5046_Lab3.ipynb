{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCHPkKbuhPF6"
      },
      "source": [
        "# Lab 03\n",
        "\n",
        "Today we will investigate some word representation models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7zRFBs2G9VL"
      },
      "source": [
        "# Key Content Summaries in WEEK2Lec\n",
        "\n",
        "*  Problems with Count-based Word Representation --> think about word similarity and dimensions/efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os4ywASdOuFu"
      },
      "source": [
        "## Word2Vec/FastText\n",
        "\n",
        "*  **SAME GOAL:** both algorithms use a neuron network model ***(either with CBOW or Skip-Gram network architecture)*** to learn vector representations of words.\n",
        "\n",
        "*  **DIFFERENT STRATEGY:** Word2Vec feeds individual words into the Neural Network, while FastText breaks words into several n-grams/sub-words. AS OOV ISSUEs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6020xqQsnFt"
      },
      "outputs": [],
      "source": [
        "## two approaches to train word embeddings are CBOW and Skip-Gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXD5azEfHMFt"
      },
      "source": [
        "## CBOW\n",
        "\n",
        "<!-- \n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1XoD5ahO2eO82KFqXsNWKSXeYVC478L6Q' />\n",
        "<!-- <figcaption>TF-IDF</figcaption> -->\n",
        "</center>\n",
        "</figure>\n",
        "(Each word is represented using one-hot encoding)\n",
        "\n",
        "```\n",
        "# C == the number of context words\n",
        "# V == the number of words in the vocabulary == len(vocab)\n",
        "# N == the number of features in hidden layer\n",
        "```\n",
        "\n",
        "**INPUT:** C Context Words >> CxV-dim \n",
        "\n",
        "**OUTPUT:** ONE Center Word >> 1xV-dim\n",
        "\n",
        "\n",
        "*  input - x: one-hot encoding representing each context word --> GOT: CxV-dim\n",
        "\n",
        "*  from input to projection1: using the input to dot product the first parameter matrix W1(VxN) --> GOT: CxN-dim\n",
        "\n",
        "*  from input to projection2: taking the mean of the output of the previous step  --> GOT: 1xN-dim\n",
        "\n",
        "*  from projection to output: using the 1xN-dim to dot product the second parameter matrix W2 --> GOT: 1xV-dim\n",
        "\n",
        "*  softmax()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ht1IsYmOi_-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zlzc_yFHOUg"
      },
      "source": [
        "## Skip-Gram\n",
        "\n",
        "\n",
        "<!-- <figure>\n",
        "<center>\n",
        "<img src='https://drive.google.com/uc?export=view&id=1tOtjNNphiZ1x5F4jO_9sKsZvK5qalDqP' />\n",
        "<!-- <figcaption>TF-IDF</figcaption> -->\n",
        "</center>\n",
        "</figure>\n",
        "(Each word is represented using one-hot encoding)\n",
        "\n",
        "```\n",
        "# C == the number of context words\n",
        "# V == the number of words in the vocabulary == len(vocab)\n",
        "# N == the number of features in hidden layer\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "**INPUT:** ONE Center Word >> 1xV-dim\n",
        "\n",
        "**OUTPUT:** C Context Words >> CxV-dim\n",
        "\n",
        "\n",
        "*  input - x: one-hot encoding representing the center word --> GOT: 1xV-dim\n",
        "\n",
        "*  from input to projection: using the input to dot product the first parameter matrix W1(VxN) --> GOT: 1xN-dim\n",
        "\n",
        "*  from projection to output: using the 1xN-dim to dot product the second parameter matrix W2(NxV), repeating this C times for each  --> GOT: CxV-dim >> THAT IS, each context word will be represented by ONE 1xV-dim vector.\n",
        "\n",
        "*  softmax() \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDk4O3auQgod"
      },
      "outputs": [],
      "source": [
        "## Before the output layer (which gives us one-hot encoding for each context word), we have a softmax step. \n",
        "## So the output from the hidden layer would be of the 1xV-dim, which will be fed into a softmax layer. \n",
        "## But we need to do softmax C times, as we have C context words, and we want to get C one-hot-encoding vectors to represent each context word. \n",
        "## That is we will get C representations. \n",
        "## The softmax outputs obtained the first time may be the same for C context words, but we are going to calculate the loss between softmax output with each context word, and then back-propagating to optimize the final results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGegzYPCKVwg"
      },
      "outputs": [],
      "source": [
        "sent = 'NLP is amazing and I love NLP very much'\n",
        "window_size = 2\n",
        "# C = the number of context words \n",
        "C_max = 4\n",
        "# center word = 1, context words = 4\n",
        "\n",
        "\n",
        "# center_wrods = 'amazing'; window_size = 2 == context words will be the two before and after the center word -- the maximum number of the context words will be 2xwindow_size\n",
        "# len(vocab) = V = 8; dimension of hidden layers (the number of features) = N = 5\n",
        "## for CBOW :: C context words >> ONE center word\n",
        "## input for CBOW = C context words == 'NLP','is','and','I' >> (4x8) \n",
        "### from input layer to hidden layer, using weight matrix W(8x5) >> (4x5)=(CxN)\n",
        "##### ** weight matrix W(8x5)\n",
        "### in hidden layer, we get mean of previous output --> (1x5)\n",
        "### from hidden layer to output layer, using weight matrix W'(5x8) --> (1x8)=(V-dim)\n",
        "##### ** weight matrix W'(5x8)\n",
        "## for Skip-Gram :: ONE center word >> C context words  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd8zGmWlRjzg"
      },
      "source": [
        "## Word2Vec\n",
        "\n",
        "***THAT IS, input will be individual words***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNyhgK5QTOuD",
        "outputId": "c54a41b9-faf3-4949-9571-445bc3ee3f8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "import re\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmae7urS8RHD"
      },
      "source": [
        "### Download data from Google Drive\n",
        "For today's lab we will download and use TED script data we are providing. The data is stored in Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV7vMHSahdnf"
      },
      "source": [
        "#### Option (1) Colab\n",
        "\n",
        "**Google Drive Access Setup**\n",
        "\n",
        "Running the following code should direct you to the Google Sign In page. Sign in with your own Google account by following the instructions on the page.\n",
        "\n",
        "After that, the code should download the file.\n",
        "\n",
        "**Downloading TED Scripts from Google Drive**\n",
        "\n",
        "Click on left side \"Files\" tab to check if the file has downloaded successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTSQtnPkfyzj"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '17tGzZyLbz1W3xedRhhl-j5i1ndgaM_yM'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')\n",
        "\n",
        "id = '1709bhW6wcZx9jnypRFNPgnY-P51OEfIJ'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.json')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewAbjQzThnT5"
      },
      "source": [
        "#### Option (2) Local\n",
        "\n",
        "Download these two files:\n",
        "- https://drive.google.com/file/d/17tGzZyLbz1W3xedRhhl-j5i1ndgaM_yM/view?usp=sharing\n",
        "- https://drive.google.com/file/d/1709bhW6wcZx9jnypRFNPgnY-P51OEfIJ/view?usp=sharing\n",
        "\n",
        "You should not need a Google Account to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIPpEvI4kqMV"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "The code below prepares our data, making several simplifications that are helpful when working with smaller datasets.\n",
        "\n",
        "#### Option (1) Run Code\n",
        "\n",
        "This can take a while sometimes, so if it does not finish within 5 minutes, use the json file instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VYmEQgB7XoDE",
        "outputId": "05bbdf2c-ab28-4485-cf53-da1ec5613747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
          ]
        }
      ],
      "source": [
        "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Get the contents of <content> tag from the xml file\n",
        "## that is, to get our text data\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# Remove \"Sound-effect labels\" using a regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Break the text into sentences using the NLTK library\n",
        "## Sentence Tokenisation\n",
        "sent_text = sent_tokenize(content_text)\n",
        "\n",
        "# Remove punctuation and change all characters to lowercase\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenise each sentence to process individual words\n",
        "## Word Tokenisations\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Print a sample of 10 (tokenised) sentences\n",
        "print(sentences[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZdHXgUXLaC6"
      },
      "source": [
        "#### Option (2) Use json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT0V4WIfLaME"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "sentences = json.load(open(\"ted_en-20160408.json\"))\n",
        "\n",
        "# Print a sample of 10 (tokenised) sentences\n",
        "print(sentences[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CojV1MbhkQxK"
      },
      "source": [
        "### Word2Vec - Continuous Bag-Of-Words (CBOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLq1VIZ7TDog"
      },
      "source": [
        "For more details about gensim.models.word2vec you can refer to the [API for Gensim Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW1iEee3lZC9"
      },
      "outputs": [],
      "source": [
        "# Initialize and train a word2vec model with the following parameters:\n",
        "\n",
        "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
        "# size: dimensionality of the word vectors \n",
        "# window_size >> it controls the number of context words\n",
        "## \n",
        "## the choice of Window Size is based on your specific data - see what size actually represents the meaning of the word well\n",
        "## Smaller window sizes (2-15) lead to embeddings where high similarity scores between two embeddings indicates that the words are interchangeable\n",
        "## Larger window sizes (15-50+) lead to embeddings where similarity is more indicative of relatedness of the words\n",
        "## \n",
        "# min_count: ignores all words with total frequency lower than the specified count value\n",
        "# workers: Use the specified number of worker threads to train the model (ie., enable faster training on multicore machines)\n",
        "\n",
        "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "\n",
        "## .wv == The trained word vectors are stored in a KeyedVectors instance, as model.wv\n",
        "vector = wv_cbow_model.wv['computer']  # get numpy vector of a word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "055e9afc-e406-402d-cf70-732759a1aa57",
        "id": "jyDtwWl7sIuY"
      },
      "source": [
        "# The trained word vectors are stored in a KeyedVectors instance as model.wv\n",
        "# Get the 10 most similar words to 'man' by calling most_similar() \n",
        "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n",
        "\n",
        "similar_words = wv_cbow_model.wv.most_similar(\"man\") # topn=10 by default\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.8533000946044922),\n",
            " ('guy', 0.8113319277763367),\n",
            " ('lady', 0.7864135503768921),\n",
            " ('boy', 0.7814817428588867),\n",
            " ('girl', 0.7682338953018188),\n",
            " ('gentleman', 0.7450353503227234),\n",
            " ('soldier', 0.7398906946182251),\n",
            " ('kid', 0.7111135721206665),\n",
            " ('friend', 0.6803527474403381),\n",
            " ('photographer', 0.6772572994232178)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsFHg0znlPSf"
      },
      "source": [
        "### Word2Vec - Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k16AowhCWUXu"
      },
      "outputs": [],
      "source": [
        "# Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2d56b0-3e1c-4c8e-dfcd-f7982c303f14",
        "id": "auJ7jeE8sNl1"
      },
      "source": [
        "similar_words = wv_sg_model.wv.most_similar(\"man\")\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('woman', 0.7846474647521973),\n",
            " ('rabbi', 0.7115237712860107),\n",
            " ('soldier', 0.7067371606826782),\n",
            " ('guy', 0.6972269415855408),\n",
            " ('boy', 0.6956959962844849),\n",
            " ('pianist', 0.6854783296585083),\n",
            " ('michelangelo', 0.6789897680282593),\n",
            " ('testament', 0.6779104471206665),\n",
            " ('dancer', 0.6662606596946716),\n",
            " ('gentleman', 0.6645552515983582)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfF7YqvpppbG"
      },
      "source": [
        "## Word2Vec vs FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8IV7D6VAEcr"
      },
      "source": [
        "Word2Vec - Skip Gram cannot find similar words to \"electrofishing\" as \"electrofishing\" is not in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "000e8969-9a8a-46ca-f00b-4224a56937be",
        "id": "l6cKwDSasPr_"
      },
      "source": [
        "## Word2Vec  ---  OOV=out of vocab  -- word-level\n",
        "## FastText - n-grams \n",
        "    ## apple  -  (2-grams) -- ap pp pl le\n",
        "    \n",
        "imilar_words  =wv_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(similar_words)\n",
        "\n",
        "# Note: When run, this code should produce an error."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-0e3cbd324a01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilar_words\u001b[0m  \u001b[0;34m=\u001b[0m\u001b[0mwv_sg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Note: When run, this code should produce an error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'electrofishing' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TpkScI8sA9G"
      },
      "source": [
        "### FastText - Skip Gram\n",
        "\n",
        "Now we will switch to FastText, which can handle out-of-vocabulary words (OOV)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAqOR1Vqps6M"
      },
      "outputs": [],
      "source": [
        "from gensim.models import FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqkvyiUw_DRh"
      },
      "outputs": [],
      "source": [
        "# We initialize and train FastText with a Skip Gram architecture (sg=1)\n",
        "ft_sg_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424d11f5-69bf-4490-c1c8-80fc9a6e0ab3",
        "id": "psaboXJVsYTw"
      },
      "source": [
        "# Now, try 'electrofishing' and we will see that FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result = ft_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('electrolux', 0.8027057647705078),\n",
            " ('electro', 0.7873678803443909),\n",
            " ('electrolyte', 0.7839568853378296),\n",
            " ('electric', 0.7741890549659729),\n",
            " ('airbus', 0.7718110084533691),\n",
            " ('electroshock', 0.7662405967712402),\n",
            " ('gastric', 0.7549958229064941),\n",
            " ('electrochemical', 0.754688560962677),\n",
            " ('electrogram', 0.7543359994888306),\n",
            " ('electrons', 0.7532363533973694)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0x2aQpfsFSx"
      },
      "source": [
        "### FastText - Continuous Bag-Of-Words (CBOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUBqvqpc2sbL"
      },
      "outputs": [],
      "source": [
        "# Now we initialize and train FastText with CBOW architecture (sg=0)\n",
        "ft_cbow_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8686ac3-d2df-4668-a1d9-090ad39e9db3",
        "id": "kGPkyYPvsd5t"
      },
      "source": [
        "# Again, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result = ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('electric', 0.9155791997909546),\n",
            " ('electro', 0.9024492502212524),\n",
            " ('electrolux', 0.8950530290603638),\n",
            " ('electron', 0.8861533403396606),\n",
            " ('electronic', 0.8847971558570862),\n",
            " ('electrolyte', 0.8803984522819519),\n",
            " ('electroshock', 0.8720880746841431),\n",
            " ('electrode', 0.8711128234863281),\n",
            " ('electrical', 0.8702925443649292),\n",
            " ('electromagnet', 0.863209068775177)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hjmOhmRi7Ov"
      },
      "source": [
        "## King - Man + Woman = ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw7b9OSwjGm0"
      },
      "source": [
        "Try using both CBOW and Skip Gram models to calculate \"King - Man + Woman = ?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovTXjSdgrw36"
      },
      "outputs": [],
      "source": [
        "# We can specify the positive/negative word list with the positive/negative parameters to create a word expression\n",
        "# Top N most similar words can be specified with the topn parameter\n",
        "result = wv_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1) ## topn=10\n",
        "print(result)\n",
        "\n",
        "## positive? negative? \n",
        "## As described in the original word2vec papers, \n",
        "##      it performs vector arithmetic: \n",
        "    ##      adding the positive vectors, \n",
        "    ##      subtracting the negative, \n",
        "    ##      then from that resulting position, \n",
        "    ##      listing the known-vectors closest to that angle.\n",
        "\n",
        "# You can think of this as, \n",
        "#   start at 'king'-vector, \n",
        "#   add 'woman'-vector, \n",
        "#   subtract 'man'-vector, \n",
        "#       from where you wind up, \n",
        "#       report ranked word-vectors closest to that point \n",
        "#       (while leaving out any of the 3 query vectors)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cd2e5c-d38a-4241-e23f-bf9b5a47bfba",
        "id": "Sjausqr8skDt"
      },
      "source": [
        "# We can specify the positive/negative word list with the positive/negative parameters to create a word expression\n",
        "# Top N most similar words can be specified with the topn parameter\n",
        "result = wv_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('president', 0.7592150568962097)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22feac29-4b4d-4c52-9027-2c77e0a208bc",
        "id": "pGrPchSFskDt"
      },
      "source": [
        "result = wv_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('luther', 0.6705532073974609)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d42677c-7004-4ef8-cd19-0db00c652659",
        "id": "yarB20H8skDt"
      },
      "source": [
        "result = ft_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('kidding', 0.89261394739151)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e2cae2-72cf-408f-cf20-69e7ebbe662d",
        "id": "tfvxCCRBskDt"
      },
      "source": [
        "result = ft_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('pauling', 0.6854696869850159)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpAd8t-wjTMA"
      },
      "source": [
        "This is not what we expected...\n",
        "\n",
        "We are using relatively little data to form our embeddings. That means the representations are not going to be as good.\n",
        "\n",
        "In the next section, we will try again with a larger training dataset. Training ourselves would be too compute intensive, so we will use vectors Google trained using Google News data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMY5w8F7rElp"
      },
      "source": [
        "## Using Pretrained word embeddings with Gensim\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keivkY13L4Nz"
      },
      "source": [
        "### 1.Download and load from Google pretrained Word2Vec binary file\n",
        "\n",
        "In case you are interested, here are links to the original code for word2vec:\n",
        "- [The Original Project](https://code.google.com/archive/p/word2vec/)\n",
        "- [GitHub Port of the original](https://github.com/tmikolov/word2vec)\n",
        "- [Another GitHub Repo](https://github.com/dav/word2vec) with an effort to update compatibility with other systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSWEK-vzCak8"
      },
      "source": [
        "#### Option (1) Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teQvZDSirVVC"
      },
      "outputs": [],
      "source": [
        "# Download the pre-trained vectors trained on part of the Google News dataset (about 100 billion words)\n",
        "# Beware, this file is big (3.39GB) - you might need to wait a while! \n",
        "id2 = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "downloaded = drive.CreateFile({'id':id2}) \n",
        "downloaded.GetContentFile('GoogleNews-vectors-negative300.bin.gz')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffqbb9lGCbiW"
      },
      "source": [
        "#### Option (2) Local\n",
        "\n",
        "Download the file from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing and save it locally.\n",
        "\n",
        "You should not need a Google Account to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vy4C3llXCeMP"
      },
      "source": [
        "### Processing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTrXl4FRMitm"
      },
      "outputs": [],
      "source": [
        "# Decompress the downloaded file\n",
        "!gzip -d /content/GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64e_sRJ1rhUa"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "# Load the pretrained vectors with KeyedVectors instance\n",
        "# Note that we set the limit=100000 here, which means we set a maximum number of word-vectors to read from the file, to avoid out of memory issues and to load the vectors faster. \n",
        "\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "gn_wv_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, limit=100000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0e8296c-930f-44e9-c1ba-12b32dc5343f",
        "id": "5ToKctmis325"
      },
      "source": [
        "# Now we can try to calculate \"King - Man + Woman = ?\" again\n",
        "result = gn_wv_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118192911148071)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also try to extract two example word embeddings and check their shape\n",
        "wv_banana = gn_wv_model[\"banana\"] \n",
        "wv_avocado = gn_wv_model[\"avocado\"]\n",
        "print(wv_banana.shape)\n",
        "print(wv_avocado.shape)\n",
        "\n",
        "# We can also calculate the cosine similarity ourselves with the extracted words\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cosine_similarity([wv_banana],[wv_avocado])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b3304c-7cfe-450f-b048-5aefad885775",
        "id": "nY28N2_xs326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n",
            "(300,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.5332662]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Ws7QvPMq9s"
      },
      "source": [
        "### 2.Load a pretrained word embedding model using the Gensim API\n",
        "The following code illustrates another way of loading pretrained word embeddings with Gensim. Here we try a GloVe embedding trained on Twitter data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4d480d-abf5-477a-eb16-3784c2c2d8cb",
        "id": "uHJvpO8itBBw"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# download the model and return it as an object ready for use\n",
        "model = api.load(\"glove-twitter-25\")  \n",
        "# The similarity() function can calculate the cosine similarity between two words\n",
        "print(model.similarity(\"cat\",\"dog\"))\n",
        "# The distance() function returns (1 - cosine similarity), which can be useful in some cases\n",
        "print(model.distance(\"cat\",\"dog\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "0.95908207\n",
            "0.040917932987213135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqLruu6247Ze"
      },
      "source": [
        "# [Tips] Play with Colab Form Fields \n",
        "**The Form** supports multiple types of fields, including **input fields**, **dropdown menus**. \n",
        "\n",
        "In Lab2 E1, we already used the input fields. Let's try more now. You can edit this section by double-clicking it. \n",
        "\n",
        "Let's get familiar by changing the value in each input field (on the right) and checking the changes in the code (on the left) - and vice versa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBNvQmee5QIG"
      },
      "outputs": [],
      "source": [
        "#@title Example form fields\n",
        "#@markdown please insert a description\n",
        "\n",
        "string = 'examples'  #@param {type: \"string\"}\n",
        "slider_value = 117  #@param {type: \"slider\", min: 100, max: 200}\n",
        "number = 102  #@param {type: \"number\"}\n",
        "date = '2020-01-05'  #@param {type: \"date\"}\n",
        "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
        "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
        "\n",
        "\n",
        "#print the output\n",
        "print(\"string is\",string)\n",
        "print('slider_value',slider_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrjbuZYrXD88"
      },
      "source": [
        "# Extension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UWjBxLdTcEi"
      },
      "source": [
        "## Word Embedding Visual Inspector (WEVI)\n",
        "If you would like to visualise how Word2Vec is learning, the following link is useful https://ronxin.github.io/wevi/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}